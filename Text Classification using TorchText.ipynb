{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classify news articles into categories using TorchText</h3>\n",
    "<p>The notebook includes the steps to build a text classifier using AG News dataset in TorchText. This also shows how to use TorchText functions to explore datasets for NLP and reduce overhead of preprocessing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:07, 15820.59lines/s]\n",
      "120000lines [00:14, 8298.87lines/s]\n",
      "7600lines [00:01, 7440.87lines/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"pytorch_text_classification\"\n",
    "NGRAMS = 2\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "# Download and load dataset\n",
    "train_dataset, test_dataset = text_classification.DATASETS[\"AG_NEWS\"](root=data_dir, ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " tensor([    572,     564,       2,    2326,   49106,     150,      88,       3,\n",
       "            1143,      14,      32,      15,      32,      16,  443749,       4,\n",
       "             572,     499,      17,      10,  741769,       7,  468770,       4,\n",
       "              52,    7019,    1050,     442,       2,   14341,     673,  141447,\n",
       "          326092,   55044,    7887,     411,    9870,  628642,      43,      44,\n",
       "             144,     145,  299709,  443750,   51274,     703,   14312,      23,\n",
       "         1111134,  741770,  411508,  468771,    3779,   86384,  135944,  371666,\n",
       "            4052]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a class for text\n",
    "class TextTopic(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.output = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.5\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.output.weight.data.uniform_(-init_range, init_range)\n",
    "        self.output.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # A row of data is array of word indexes\n",
    "        # text is concat of index arrays in a batch of data\n",
    "        # If there are three texts with lengths 3, 4, 6\n",
    "        # then, length of text: 13 and offsets: [0, 3, 7]\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        output = self.output(embedded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EMBED_DIM = 32\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class TextClassifier(object):\n",
    "    def __init__(self, train_dataset, test_dataset):\n",
    "        # storage of all words in a train dataset\n",
    "        self.vocab = train_dataset.get_vocab()\n",
    "        \n",
    "        # There are 4 classes in ag-news-dataset\n",
    "        self.num_classes = len(train_dataset.get_labels())\n",
    "        \n",
    "        train_len = int(len(train_dataset) * 0.9)\n",
    "        valid_len = len(train_dataset) - train_len\n",
    "        \n",
    "        self.train_data, self.valid_data = random_split(train_dataset, [train_len, valid_len])\n",
    "        self.test_data = test_dataset\n",
    "        print(\"Train data: {0}, Validation data: {1}, Test data: {2}\"\n",
    "              .format(len(self.train_data), len(self.valid_data), len(self.test_data)))\n",
    "\n",
    "        # Initialize model, optimizer and loss\n",
    "        self.model = TextTopic(len(self.vocab), EMBED_DIM, self.num_classes)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=4.0)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.9)\n",
    "        \n",
    "    def generate_batch(self, batch):\n",
    "        # Generate features for one batch\n",
    "        label = torch.tensor([entry[0] for entry in batch])\n",
    "        text = [entry[1] for entry in batch]\n",
    "        \n",
    "        offsets = [0] + [len(entry) for entry in text]\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "\n",
    "        text = torch.cat(text)\n",
    "        return text, offsets, label    \n",
    "\n",
    "    def train_util(self):\n",
    "        train_loss, train_acc = 0, 0        \n",
    "        data = DataLoader(self.train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=self.generate_batch)\n",
    "\n",
    "        for i, (text, offsets, cls) in enumerate(data):\n",
    "            # Train the model with one batch of data\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self.model(text, offsets)\n",
    "            loss = self.criterion(output, cls)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "        # Adjust the learning rate\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return train_loss / len(self.train_data), train_acc / len(self.train_data)\n",
    "\n",
    "    def evaluate_util(self, test_data):\n",
    "        loss, acc = 0, 0\n",
    "        data = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=self.generate_batch)\n",
    "\n",
    "        for text, offsets, cls in data:\n",
    "            # Evaluate model without gradient\n",
    "            with torch.no_grad():\n",
    "                output = self.model(text, offsets)\n",
    "                loss = self.criterion(output, cls)\n",
    "                loss += loss.item()\n",
    "                acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "        return loss / len(test_data), acc / len(test_data)    \n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = self.train_util()\n",
    "            valid_loss, valid_acc = self.evaluate_util(self.valid_data)\n",
    "\n",
    "            secs = int(time.time() - start_time)\n",
    "            mins = secs / 60\n",
    "            secs = secs % 60\n",
    "\n",
    "            print('\\nEpoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "            print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "            print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        print('Checking the results of test dataset...')\n",
    "        test_loss, test_acc = self.evaluate_util(self.test_data)\n",
    "        print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')\n",
    "        \n",
    "    def predict(self, text, ngrams=2):\n",
    "        tokenizer = get_tokenizer(\"basic_english\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Create feature row for prediction\n",
    "            text = [self.vocab[token] for token in ngrams_iterator(tokenizer(text), ngrams)]\n",
    "\n",
    "            # Get output from the model\n",
    "            output = self.model(torch.tensor(text), torch.tensor([0]))\n",
    "            result = output.argmax(1).item() + 1 \n",
    "            \n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 108000, Validation data: 12000, Test data: 7600\n",
      "\n",
      "Epoch: 1  | time in 0 minutes, 21 seconds\n",
      "\tLoss: 0.0264(train)\t|\tAcc: 84.5%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.0%(valid)\n",
      "\n",
      "Epoch: 2  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0119(train)\t|\tAcc: 93.6%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.8%(valid)\n",
      "\n",
      "Epoch: 3  | time in 0 minutes, 22 seconds\n",
      "\tLoss: 0.0069(train)\t|\tAcc: 96.4%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 90.5%(valid)\n",
      "\n",
      "Epoch: 4  | time in 0 minutes, 21 seconds\n",
      "\tLoss: 0.0038(train)\t|\tAcc: 98.2%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 91.0%(valid)\n",
      "\n",
      "Epoch: 5  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0022(train)\t|\tAcc: 99.0%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 91.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(train_dataset, test_dataset)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0003(test)\t|\tAcc: 90.7%(test)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "text = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "print(\"This is a %s news\" % ag_news_label[model.predict(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
